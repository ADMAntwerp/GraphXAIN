{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from itertools import combinations\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from utils.utils import (\n",
    "    predict_reg,\n",
    "    get_source_node_labels_as_dict_with_titles,\n",
    "    get_movie_stats,\n",
    "    df_column_descriptions,\n",
    "    device, \n",
    "    set_seed\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "# Constants\n",
    "MODEL_PATH = \"movie_gcn_model.pt\"\n",
    "SEED = 42\n",
    "EPOCHS = 7500\n",
    "HIDDEN_DIM = 32\n",
    "LR = 0.01\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "\n",
    "def preprocess_dataframe(path):\n",
    "    \"\"\"Load and preprocess the DataFrame with basic features only\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df.rename(columns={\"Runtime\": \"Duration\"}, inplace=True)\n",
    "    # Keep only essential columns\n",
    "    essential_cols = [\n",
    "        \"Series_Title\",\n",
    "        \"Overview\",\n",
    "        \"Released_Year\",\n",
    "        \"Duration\",\n",
    "        \"Genre\",\n",
    "        \"IMDB_Rating\",\n",
    "        \"Meta_score\",\n",
    "        \"No_of_Votes\",\n",
    "        \"Gross\",\n",
    "        \"Certificate\",\n",
    "        \"Director\",\n",
    "        \"Star1\",\n",
    "        \"Star2\",\n",
    "        \"Star3\",\n",
    "        \"Star4\",\n",
    "    ]\n",
    "    df = df[essential_cols]\n",
    "\n",
    "    # Basic cleaning\n",
    "    df[\"Released_Year\"] = pd.to_numeric(\n",
    "        df[\"Released_Year\"].replace(\"PG\", \"1995\"), errors=\"coerce\"\n",
    "    )\n",
    "    df[\"Duration\"] = pd.to_numeric(\n",
    "        df[\"Duration\"].str.replace(\" min\", \"\"), errors=\"coerce\"\n",
    "    )\n",
    "    # Convert Gross to numeric (remove commas)\n",
    "    if df[\"Gross\"].dtype == object:\n",
    "        df[\"Gross\"] = df[\"Gross\"].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "        df[\"Gross\"] = pd.to_numeric(df[\"Gross\"], errors=\"coerce\")\n",
    "\n",
    "    # Convert No_of_Votes to numeric (remove commas)\n",
    "    if df[\"No_of_Votes\"].dtype == object:\n",
    "        df[\"No_of_Votes\"] = (\n",
    "            df[\"No_of_Votes\"].astype(str).str.replace(\",\", \"\", regex=True)\n",
    "        )\n",
    "        df[\"No_of_Votes\"] = pd.to_numeric(df[\"No_of_Votes\"], errors=\"coerce\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=[\"IMDB_Rating\"])  # Must have target\n",
    "    df = df.fillna(\n",
    "        {\n",
    "            \"Released_Year\": df[\"Released_Year\"].median(),\n",
    "            \"Duration\": df[\"Duration\"].median(),\n",
    "            \"Meta_score\": df[\"Meta_score\"].median(),\n",
    "            \"Gross\": df[\"Gross\"].median(),\n",
    "            \"No_of_Votes\": df[\"No_of_Votes\"].median(),\n",
    "            \"Director\": \"Unknown\",\n",
    "            \"Certificate\": \"Unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create features including normalized movie counts for actors\"\"\"\n",
    "    feature_names = []\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_features = df[\n",
    "        [\"Released_Year\", \"Duration\", \"Meta_score\", \"Gross\", \"No_of_Votes\"]\n",
    "    ].values\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_features = scaler.fit_transform(numeric_features)\n",
    "    feature_names.extend(\n",
    "        [\"Released_Year\", \"Duration\", \"Meta_score\", \"Gross\", \"No_of_Votes\"]\n",
    "    )\n",
    "\n",
    "    # Genre encoding\n",
    "    genre_encoder = LabelEncoder()\n",
    "    genre_encoded = genre_encoder.fit_transform(\n",
    "        df[\"Genre\"].str.split(\",\").str[0]\n",
    "    ).reshape(-1, 1)\n",
    "    feature_names.append(\"Genre\")\n",
    "\n",
    "    # Certificate\n",
    "    certificate_encoder = LabelEncoder()\n",
    "    certificate_encoded = certificate_encoder.fit_transform(df[\"Certificate\"]).reshape(\n",
    "        -1, 1\n",
    "    )\n",
    "    feature_names.append(\"Certificate\")\n",
    "\n",
    "    # Director encoding\n",
    "    dir_encoder = LabelEncoder()\n",
    "    dir_encoded = dir_encoder.fit_transform(df[\"Director\"]).reshape(-1, 1)\n",
    "    feature_names.append(\"Director\")\n",
    "\n",
    "    # Actor movie count features\n",
    "    def create_actor_counts(df, actor_cols=[\"Star1\", \"Star2\", \"Star3\", \"Star4\"]):\n",
    "        \"\"\"Actor features: Count of movies each actor appears in (normalized)\"\"\"\n",
    "        # Count total movies for each actor across all positions\n",
    "        all_actors = pd.Series([actor for col in actor_cols for actor in df[col]])\n",
    "        movie_counts = all_actors.value_counts()\n",
    "\n",
    "        # Create feature matrix for actor counts\n",
    "        actor_count_features = np.zeros((len(df), len(actor_cols)))\n",
    "\n",
    "        # Fill in the counts for each actor position\n",
    "        for i, col in enumerate(actor_cols):\n",
    "            actor_count_features[:, i] = df[col].map(movie_counts).fillna(0)\n",
    "\n",
    "        # Normalize the counts\n",
    "        scaler = MinMaxScaler()\n",
    "        actor_count_features = scaler.fit_transform(actor_count_features)\n",
    "\n",
    "        return actor_count_features\n",
    "\n",
    "    # Generate actor features\n",
    "    actor_features = create_actor_counts(df)\n",
    "    feature_names.extend(\n",
    "        [\n",
    "            \"Star1_appearances\",\n",
    "            \"Star2_appearances\",\n",
    "            \"Star3_appearances\",\n",
    "            \"Star4_appearances\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Combine all features\n",
    "    node_features = np.hstack(\n",
    "        [\n",
    "            numeric_features,\n",
    "            genre_encoded,\n",
    "            dir_encoded,\n",
    "            certificate_encoded,\n",
    "            actor_features,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return node_features, df[\"IMDB_Rating\"].values, feature_names\n",
    "\n",
    "\n",
    "def create_graph_edges(df):\n",
    "    \"\"\"Create edges based on shared actors\"\"\"\n",
    "    actor_cols = [\"Star1\", \"Star2\", \"Star3\", \"Star4\"]\n",
    "    actor_to_movies = {}\n",
    "\n",
    "    # Map actors to movies\n",
    "    for i, row in df.iterrows():\n",
    "        for col in actor_cols:\n",
    "            actor = str(row[col]).strip()\n",
    "            if actor != \"Unknown\" and actor != \"nan\":\n",
    "                if actor not in actor_to_movies:\n",
    "                    actor_to_movies[actor] = []\n",
    "                actor_to_movies[actor].append(i)\n",
    "\n",
    "    # Create edges between movies with shared actors\n",
    "    edges = set()\n",
    "    for movies in actor_to_movies.values():\n",
    "        if len(movies) > 1:\n",
    "            for m1, m2 in combinations(movies, 2):\n",
    "                edges.add((m1, m2))\n",
    "                edges.add((m2, m1))\n",
    "\n",
    "    if not edges:  # If no edges found, create self-loops\n",
    "        edges = {(i, i) for i in range(len(df))}\n",
    "\n",
    "    return torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "\n",
    "class GCNRegressor(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats=1, dropout=0.5):\n",
    "        super(GCNRegressor, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_feats)\n",
    "        self.conv2 = GCNConv(hidden_feats, out_feats)\n",
    "        self.dropout = dropout\n",
    "        self.in_feats = in_feats\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, data, optimizer, criterion, model_path, patience=500):\n",
    "    \"\"\"Train the model with early stopping\"\"\"\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index).squeeze(-1)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data.x, data.edge_index).squeeze(-1)\n",
    "            val_rmse = torch.sqrt(\n",
    "                F.mse_loss(val_out[data.val_mask], data.y[data.val_mask])\n",
    "            )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_model_state = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"val_rmse\": val_rmse.item(),\n",
    "                \"in_features\": model.in_feats,\n",
    "                \"hidden_features\": HIDDEN_DIM,\n",
    "            }\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:04d} | Train Loss: {loss.item():.4f} | Val RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Save the best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, model_path)\n",
    "\n",
    "    return (\n",
    "        best_model_state[\"model_state_dict\"] if best_model_state is not None else None\n",
    "    )\n",
    "\n",
    "\n",
    "def create_data_splits(num_nodes, seed=42):\n",
    "    \"\"\"Create train/val/test splits\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    train_count = int(0.6 * num_nodes)\n",
    "    val_count = int(0.2 * num_nodes)\n",
    "\n",
    "    train_idx = indices[:train_count]\n",
    "    val_idx = indices[train_count : train_count + val_count]\n",
    "    test_idx = indices[train_count + val_count :]\n",
    "\n",
    "    masks = []\n",
    "    for idx in [train_idx, val_idx, test_idx]:\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = True\n",
    "        masks.append(mask)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Load and preprocess data\n",
    "df = preprocess_dataframe(\n",
    "    \"datasets/IMDB/imdb_top_1000.csv\"\n",
    ")\n",
    "\n",
    "# Create features and graph structure\n",
    "node_features, y, header = create_features(df)\n",
    "edge_index = create_graph_edges(df)\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "x = torch.tensor(node_features, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "df.reset_index(drop=True, inplace=True)  # ensure indices are 0..N-1\n",
    "\n",
    "# Create a dictionary mapping node index -> movie title\n",
    "node_index_to_title = {\n",
    "    idx: row[\"Series_Title\"] for idx, row in df.iterrows()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1000\n",
      "Number of edges: 5608\n",
      "Number of node features: 12\n",
      "Loading existing model...\n",
      "Loaded model with validation RMSE: 0.2670\n",
      "\n",
      "Test RMSE: 0.2808\n"
     ]
    }
   ],
   "source": [
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Print graph information\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of node features: {data.num_node_features}\")\n",
    "\n",
    "# Create data splits\n",
    "train_mask, val_mask, test_mask = create_data_splits(data.num_nodes, SEED)\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GCNRegressor(in_feats=data.num_node_features, hidden_feats=HIDDEN_DIM)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Loading existing model...\")\n",
    "    checkpoint = torch.load(MODEL_PATH)\n",
    "\n",
    "    if checkpoint[\"in_features\"] == data.num_node_features:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Loaded model with validation RMSE: {checkpoint['val_rmse']:.4f}\")\n",
    "    else:\n",
    "        print(\"Warning: Existing model architecture doesn't match current data.\")\n",
    "        print(\"Training new model...\")\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "        criterion = nn.MSELoss()\n",
    "        best_model_state = train_model(\n",
    "            model, data, optimizer, criterion, MODEL_PATH, patience=500\n",
    "        )\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "else:\n",
    "    print(\"No existing model found. Training new model...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_model_state = train_model(\n",
    "        model, data, optimizer, criterion, MODEL_PATH, patience=500\n",
    "    )\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(data.x, data.edge_index).squeeze(-1)\n",
    "    test_rmse = torch.sqrt(F.mse_loss(test_out[data.test_mask], data.y[data.test_mask]))\n",
    "    print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNRegressor(\n",
       "  (conv1): GCNConv(12, 32)\n",
       "  (conv2): GCNConv(32, 1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[744, 317, 985, 455, 256]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices = torch.nonzero(data.test_mask, as_tuple=False).squeeze()\n",
    "\n",
    "set_seed(42)\n",
    "sampled_test_indices = test_indices[torch.randperm(test_indices.size(0))[:25]]\n",
    "sampled_test_indices = list(sampled_test_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node corresponds to the movie: \n",
      "\tThe Grapes of Wrath\n"
     ]
    }
   ],
   "source": [
    "# Example usage: print out the title for node:\n",
    "print(f\"Node corresponds to the movie: \\n\\t{node_index_to_title[sampled_test_indices[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation for node 95\n",
      "Generating explanation for node 432\n"
     ]
    }
   ],
   "source": [
    "# Data for LLM input\n",
    "ml_task_info = (\n",
    "    \"Graph regression task aimed at predicting the IMDB rating score of movie. \"\n",
    "    \"Each node in the graph represents a single movie, and the model predicts a continuous \"\n",
    "    \"rating value ranging from 1 to 10, where 1 signifies a movie that users did not like, \"\n",
    "    \"and 10 signifies a highly liked movie.\"\n",
    ")\n",
    "\n",
    "dataset_info = (\n",
    "    \"The graph dataset comprises 1,000 IMDB movies, featuring combined metadata such as genres, \"\n",
    "    \"release year, director and cast information, budget, and box office revenue. Additional features \"\n",
    "    \"include user ratings, number of reviews, and runtime. Graph edges represent connections between \"\n",
    "    \"movies that share at least one common actor, highlighting the collaborative network within the film industry. \"\n",
    "    \"In this undirected graph, each node represents an IMDB movie with associated features, and edges represent \"\n",
    "    \"unweighted connections through shared actors with other movies.\"\n",
    ")\n",
    "\n",
    "node_description = \"Movie from the IDBM dataset\"\n",
    "\n",
    "edges_description = \"Connections between movies that share at least one common actor\"\n",
    "\n",
    "# Narrative\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "# LLM function\n",
    "def explain_model_prediction(\n",
    "    ml_task_info,\n",
    "    dataset_info,\n",
    "    node_index,\n",
    "    node_description,\n",
    "    edges_description,\n",
    "    ### Target Node information:\n",
    "    general_llm_input,\n",
    "    main_llm_input_cont,\n",
    "    main_llm_input_cat,\n",
    "    ### Target Subgraph information:\n",
    "    edges_df_str,\n",
    "    # subgraph_feature_importance,\n",
    "    subgraph_labels,\n",
    "    model_prediction,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1.0,\n",
    "    sentence_limit=8,\n",
    "    num_feat=7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a textual explanation of why an AI model made a certain prediction.\n",
    "\n",
    "    Parameters:\n",
    "        ml_task_info (str): Information about the task.\n",
    "        dataset_info (str): Information about the dataset.\n",
    "        nodes_description (str): Description of the node.\n",
    "        edges_description (str): Description of the edges.\n",
    "        node_index (int): Target's node index.\n",
    "        ### Target Node information ###\n",
    "        general_llm_input (str): Target's node general information dataframe changed to string input.\n",
    "        main_llm_input_cont (str): Target's node continuous information dataframe changed to string input.\n",
    "        main_llm_input_cat (str): Target's node categorical information dataframe changed to string input.\n",
    "        ### Target Subgraph information ###\n",
    "        edges_df_str (str): Source, destination and edges' importance weights of the influential nodes in a subgraph.\n",
    "        # subgraph_feature_importance (dict): Dictionary with feature importance of the subgraph nodes.\n",
    "        subgraph_labels (dict): Dictionary with the node index and label/value of the subgraph nodes.\n",
    "        model_prediction (dict): Dictionary with the target node index, model's predicted probability, and predicted class.\n",
    "        model (str, optional): The GPT model name to use. Default: \"gpt-4o\".\n",
    "        temperature (float, optional): Controls the randomness of the output. Default: 1.0.\n",
    "        sentence_limit (int, optional): The maximum number of sentences. Default: 8.\n",
    "        num_feat (int, optional): The top-K most important features in the narrative. Default: 7.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated textual explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Your goal is to generate a textual explanation or narrative explaining why a graph explainer produced a certain target node's explanation subgraph and feature importance for a Graph Neural Network (GNN) model's prediction of a target node instance.\n",
    "\n",
    "    To achieve this, you will be provided with the following information:\n",
    "\n",
    "    - **Machine Learning Task Information**: Details about the machine learning task.\n",
    "    - **Dataset Information**: Information about the dataset used.\n",
    "    - **Target Node's General Information**: General information about target node.\n",
    "    - **Target Node's Data**: Information about target node's continuous features: feature name, feature importance, feature continuous value, percentile, and feature description.\n",
    "    - **Target Node's Categorical Data**: Information about target node's categorical features: feature name, feature importance, feature value, and feature description.\n",
    "    - **Target Subgraph's Edges' Influence Importance**: Dictionary with the target node's subgraph edge's influence importance on the target node prediction. The key is a neighboring node number, and the value represents the importance of the connection of the neighboring node for the target's final prediction, sorted in descending order.\n",
    "    - **Target Subgraph's Nodes' Values**: Dictionary with the target's subgraph nodes' values, where the key is a node index in a subgraph, and the value is the node's value.\n",
    "    - **Model's Prediction**: Dictionary with the target node index and predicted value.\n",
    "\n",
    "    ### Machine Learning Task Information:\n",
    "    {ml_task_info}\n",
    "\n",
    "    ### Dataset Information:\n",
    "    {dataset_info}\n",
    "\n",
    "    ### Target Node's General Information:\n",
    "    {general_llm_input}\n",
    "\n",
    "    ### Target Node's Data:\n",
    "    {main_llm_input_cont}\n",
    "\n",
    "    ### Target Node's Categorical Data:\n",
    "    {main_llm_input_cat}\n",
    "\n",
    "    ### Target's Subgraph Edge's Importance Weights:\n",
    "    {edges_df_str}\n",
    "\n",
    "    ### Target's Subgraph Nodes' Values:\n",
    "    {subgraph_labels}\n",
    "\n",
    "    ### Model's Prediction:\n",
    "    {model_prediction}\n",
    "\n",
    "    Generate a fluent and cohesive narrative that explains the prediction made by the model. In your answer, follow these rules:\n",
    "\n",
    "    **Format-related rules**:\n",
    "\n",
    "    1) Start the explanation immediately.\n",
    "    2) Limit the entire answer to {sentence_limit} sentences or fewer.\n",
    "    3) Only mention the top {num_feat} most important features in the narrative.\n",
    "    4) Do not use tables or lists, or simply rattle through the features and/or nodes one by one. The goal is to have a narrative/story.\n",
    "\n",
    "    **Content related rules**:\n",
    "\n",
    "    1) Be clear about what the model actually predicted for the target node with index {node_index}.\n",
    "    2) Discuss how the features and/or nodes contributed to final prediction. Make sure to clearly establish this the first time you refer to a feature or node.\n",
    "    3) Discuss how the subgraph's edge importance contribute to final prediction. Make sure to clearly establish this the first time you refer to an edge connection.\n",
    "    4) Consider the feature importance, feature values, averages, and percentiles when referencing their relative importance.\n",
    "    5) Begin the discussion of features by presenting those with the highest feature importance values first. The reader should be able to tell what the order of importance of the features is based on their feature importance value.\n",
    "    6) Provide a suggestion or interpretation as to why a feature contributed in a certain direction. Try to introduce external knowledge that you might have.\n",
    "    7) If there is no simple explanation for the effect of a feature, consider the context of other features and/or nodes in the interpretation.\n",
    "    8) Do not use the feature importance numeric values in your answer.\n",
    "    9) You can use the feature values themselves in the explanation.\n",
    "    10) Do not refer to the average and/or percentile for every single feature; reserve it for features where it truly clarifies the explanation.\n",
    "    11) When discussing the connections between the nodes, relate how the influence of a node's relationship might impact final prediction.\n",
    "    12) When you refer to node and edges, keep in mind that the target node is a {node_description} and edges are {edges_description} in this dataset.\n",
    "    13) Tell a clear and engaging story, including details from both feature values and node connections, to make the explanation more relatable and interesting.\n",
    "    14) Use clear and simple language that a general audience can understand, avoiding overly technical jargon or explaining any necessary technical terms in plain language.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the explanation\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # print(prompt)\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def generate_explanations_for_all_nodes(\n",
    "    model,\n",
    "    data,\n",
    "    sampled_test_indices,\n",
    "    df,\n",
    "    df_column_descriptions,\n",
    "    header,\n",
    "    get_movie_stats,\n",
    "    predict_reg,\n",
    "    get_source_node_labels_as_dict_with_titles,\n",
    "    explain_model_prediction,\n",
    "    device,\n",
    "    ml_task_info,\n",
    "    dataset_info,\n",
    "    node_description,\n",
    "    edges_description,\n",
    "    output_dir=\"../\",\n",
    "    model_name=\"GCN_Reg\",\n",
    "    topk_features=None,  # If None, defaults to using all features.\n",
    "    topk_subgraph_features=7,\n",
    "    sentence_limit=8,\n",
    "    num_feat=7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate explanations for each node in sampled_test_indices and store the results in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained GNN model.\n",
    "        data: PyG data object.\n",
    "        sampled_test_indices: List or array of node indices to explain.\n",
    "        df: DataFrame with node metadata for the IMDB dataset.\n",
    "        df_column_descriptions: Dict mapping column names to their descriptions.\n",
    "        header: List of feature names for the node attributes.\n",
    "        get_movie_stats: Function that returns continuous and categorical movie stats (from your code).\n",
    "        predict_reg: Function to predict the node's label/value from the model.\n",
    "        get_source_node_labels_as_dict_with_titles: Function that returns subgraph node labels as a dict.\n",
    "        explain_model_prediction: The LLM explanation function you defined.\n",
    "        device: torch device.\n",
    "        ml_task_info: String with ML task info.\n",
    "        dataset_info: String with dataset info.\n",
    "        node_description: String describing the node (e.g. \"Movie from the IMDB dataset\").\n",
    "        edges_description: String describing edges.\n",
    "        output_dir: Directory to store generated artifacts.\n",
    "        model_name: Name of the trained model.\n",
    "        topk_features: Number of top features to consider in explanation. If None, uses all.\n",
    "        topk_subgraph_features: Same logic for subgraph features if needed.\n",
    "        sentence_limit: The max number of sentences in the LLM explanation.\n",
    "        num_feat: The top-K most important features in the narrative.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with columns:\n",
    "        - 'node_index'\n",
    "        - 'explanation' (the textual explanation from the LLM)\n",
    "    \"\"\"\n",
    "\n",
    "    if topk_features is None:\n",
    "        topk_features = 7\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # Initialize the explainer (do once outside the loop for efficiency)\n",
    "    explainer_gnnx = Explainer(\n",
    "        model=model,\n",
    "        algorithm=GNNExplainer(epochs=200),\n",
    "        explanation_type=\"model\",\n",
    "        node_mask_type=\"common_attributes\",\n",
    "        edge_mask_type=\"object\",\n",
    "        model_config=dict(\n",
    "            mode=\"regression\",\n",
    "            task_level=\"node\",\n",
    "            return_type=\"raw\",\n",
    "        ),\n",
    "        threshold_config=dict(\n",
    "            threshold_type=\"topk\",\n",
    "            value=topk_subgraph_features,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    for idx in sampled_test_indices:\n",
    "        node_index = int(idx)\n",
    "        print(f\"Generating explanation for node {node_index}\")\n",
    "\n",
    "        # Generate explanation\n",
    "        explanation_gnnx = explainer_gnnx(data.x, data.edge_index, index=node_index)\n",
    "\n",
    "        # Visualize subgraph and feature importance (optional)\n",
    "        path_graph = os.path.join(\n",
    "            output_dir,\n",
    "            f\"explanations/IMDB/GNNX_SubG_node_{node_index}_{topk_features}_{model_name}.pdf\",\n",
    "        )\n",
    "\n",
    "        g, edges_df = explanation_gnnx.visualize_graph(\n",
    "            path=path_graph, backend=\"graphviz\", node_labels=None\n",
    "        )\n",
    "\n",
    "        topk_FI = 7\n",
    "        path_features = os.path.join(\n",
    "            output_dir,\n",
    "            f\"explanations/IMDB/GNNX_FI_node_{node_index}_{topk_FI}_{model_name}.png\",\n",
    "        )\n",
    "        fi_plot, df_score_fi = explanation_gnnx.visualize_feature_importance(\n",
    "            path=path_features,\n",
    "            feat_labels=header,\n",
    "            top_k=topk_FI,\n",
    "        )\n",
    "\n",
    "        edges_df_str = edges_df.to_string()\n",
    "\n",
    "        # Get node stats\n",
    "        stats_df, categorical_df = get_movie_stats(df, int(node_index))\n",
    "\n",
    "        stats_df[\"Description\"] = stats_df.index.map(df_column_descriptions)\n",
    "        categorical_df[\"Description\"] = categorical_df.index.map(df_column_descriptions)\n",
    "\n",
    "        # Map feature importance\n",
    "        categorical_df[\"feature_importance\"] = categorical_df.index.map(\n",
    "            df_score_fi.set_index(\"feature_name\")[\"feature_importance\"]\n",
    "        )\n",
    "\n",
    "        stats_df[\"feature_importance\"] = stats_df.index.map(\n",
    "            df_score_fi.set_index(\"feature_name\")[\"feature_importance\"]\n",
    "        )\n",
    "\n",
    "        # Reformat stats_df\n",
    "        stats_df.reset_index(inplace=True)\n",
    "        stats_df.rename(columns={\"index\": \"Feature_name\"}, inplace=True)\n",
    "        stats_df.rename(\n",
    "            columns={\n",
    "                \"Feature_name\": \"Feature Name\",\n",
    "                \"Value\": \"Feature Value\",\n",
    "                \"feature_importance\": \"Feature Importance\",\n",
    "                \"Description\": \"Feature Description\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        cont_order = [\n",
    "            \"Feature Name\",\n",
    "            \"Feature Importance\",\n",
    "            \"Feature Value\",\n",
    "            \"Percentile\",\n",
    "            \"Feature Description\",\n",
    "        ]\n",
    "        stats_df = stats_df[cont_order]\n",
    "        main_llm_input_cont = stats_df.to_string()\n",
    "\n",
    "        # Reformat categorical_df\n",
    "        categorical_df.reset_index(inplace=True)\n",
    "        categorical_df.rename(\n",
    "            columns={\n",
    "                \"index\": \"Feature Name\",\n",
    "                \"Categories\": \"Feature Value\",\n",
    "                \"feature_importance\": \"Feature Importance\",\n",
    "                \"Description\": \"Feature Description\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        cat_order = [\n",
    "            \"Feature Name\",\n",
    "            \"Feature Importance\",\n",
    "            \"Feature Value\",\n",
    "            \"Feature Description\",\n",
    "        ]\n",
    "        categorical_df = categorical_df[cat_order]\n",
    "\n",
    "        if \"Feature Importance\" in categorical_df.columns:\n",
    "            df_w_fi = categorical_df[\n",
    "                categorical_df[\"Feature Importance\"].notna()\n",
    "            ].copy()\n",
    "            df_w_fi.rename(columns={\"Feature Value\": \"Feature_values\"}, inplace=True)\n",
    "            df_wo_if = categorical_df[\n",
    "                categorical_df[\"Feature Importance\"].isna()\n",
    "            ].copy()\n",
    "            df_wo_if.drop([\"Feature Importance\"], axis=1, inplace=True)\n",
    "            df_wo_if.rename(columns={\"Feature Value\": \"Feature_values\"}, inplace=True)\n",
    "        else:\n",
    "            # If for some reason Feature Importance doesn't exist, handle gracefully\n",
    "            df_w_fi = pd.DataFrame(columns=categorical_df.columns)\n",
    "            df_wo_if = categorical_df.copy()\n",
    "\n",
    "        main_llm_input_cat = df_w_fi.to_string()\n",
    "        general_llm_input = df_wo_if.to_string()\n",
    "\n",
    "        # Get model prediction\n",
    "        model_prediction = predict_reg(model, data, node_index, device)\n",
    "\n",
    "        # Subgraph labels\n",
    "        subgraph_labels = get_source_node_labels_as_dict_with_titles(\n",
    "            edges_df, data, \"IMDB\", df\n",
    "        )\n",
    "\n",
    "        # Call LLM explanation\n",
    "        explanation = explain_model_prediction(\n",
    "            ml_task_info,\n",
    "            dataset_info,\n",
    "            node_index,\n",
    "            node_description,\n",
    "            edges_description,\n",
    "            general_llm_input,\n",
    "            main_llm_input_cont,\n",
    "            main_llm_input_cat,\n",
    "            edges_df_str,\n",
    "            subgraph_labels,\n",
    "            model_prediction,\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=1.0,\n",
    "            sentence_limit=sentence_limit,\n",
    "            num_feat=num_feat,\n",
    "        )\n",
    "\n",
    "        results.append({\"node_index\": node_index, \"explanation\": explanation})\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df_explanations = pd.DataFrame(results)\n",
    "    return df_explanations\n",
    "\n",
    "sampled_test_indices = [95, 432]\n",
    "\n",
    "df_all_explanations = generate_explanations_for_all_nodes(\n",
    "    model=model,\n",
    "    data=data,\n",
    "    sampled_test_indices=sampled_test_indices,\n",
    "    df=df,\n",
    "    df_column_descriptions=df_column_descriptions,\n",
    "    header=header,\n",
    "    get_movie_stats=get_movie_stats,\n",
    "    predict_reg=predict_reg,\n",
    "    get_source_node_labels_as_dict_with_titles=get_source_node_labels_as_dict_with_titles,\n",
    "    explain_model_prediction=explain_model_prediction,\n",
    "    device=device,\n",
    "    ml_task_info=ml_task_info,\n",
    "    dataset_info=dataset_info,\n",
    "    node_description=node_description,\n",
    "    edges_description=edges_description,\n",
    "    output_dir=\"../\",\n",
    "    model_name=\"GCN_Reg\",\n",
    "    sentence_limit=8,\n",
    "    num_feat=7,\n",
    "    topk_subgraph_features=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_index</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>The Graph Neural Network model predicted a sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>432</td>\n",
       "      <td>The model predicted a score of 7.96 for the mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_index                                        explanation\n",
       "0          95  The Graph Neural Network model predicted a sco...\n",
       "1         432  The model predicted a score of 7.96 for the mo..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Graph Neural Network model predicted a score of 7.92 for the movie \"Amélie\" (node index 95) in its task of estimating an IMDB rating based on various features and relationships with other movies. \"Amélie's\" director, Jean-Pierre Jeunet, is a significant factor in the prediction due to his well-regarded filmography, which often yields positive critic reviews. The movie's Metascore also contributes notably, although it is lower than many in the industry, indicating that other factors helped bolster the prediction. The extensive audience engagement, with a high number of votes, underscores the film's popularity, probably influencing the prediction towards a favorable outcome.\n",
      "\n",
      "The Gross revenue adds weight to the prediction, suggesting commercial success and audience reception aligned with a higher rating. Released in 2001, \"Amélie\" falls within a time when many classic films emerged, possibly aiding its favorable standing. The film's universal classification (U) suggests widespread accessibility, aligning with the positive prediction. The strongest connection in the movie's network comes from \"The Silence of the Lambs,\" a highly rated film, reflecting the shared actor network's powerful influence—emphasizing the social network effects within this dataset. Overall, \"Amélie's\" narrative charm, strong industry connections, and these diverse features collectively contribute to its predicted rating.\n"
     ]
    }
   ],
   "source": [
    "print(df_all_explanations.iloc[0]['explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicted a score of 7.96 for the movie node with index 432, which is titled \"8½.\" This prediction was significantly influenced by the movie's box office revenue, which is notably low at $50,690, perhaps indicating a limited commercial reach despite possibly high artistic value. Additionally, the movie's runtime of 138 minutes is relatively long, falling into the upper percentile range, which might hint at a more complex or experimental narrative style, common in higher-rated, critically acclaimed films. The movie's genre, Drama, also holds weight in the prediction, as dramatic films often receive higher critical acclaim.\n",
      "\n",
      "The interconnection of \"8½\" with other prominent films, such as \"La dolce vita\" and \"Fitzcarraldo,\" both boasting high ratings above 8.0, suggests a shared audience appreciation which may have bolstered its rating. These shared connections highlight the collaborative nature of the industry, with shared cast potentially increasing exposure and credibility. Meanwhile, the director of \"8½,\" Federico Fellini, is renowned for his cinematic contributions, likely adding to the film's perceived value. Finally, despite having more than 108,844 user votes, the lower engagement compared to other popular films could suggest a niche audience, frequently associated with cult classics which are celebrated for artistic rather than commercial success. Thus, combining these influential features and interconnected movie networks, the model crafted a nuanced prediction reflecting a balance between critical acclaim and collective film industry dynamics.\n"
     ]
    }
   ],
   "source": [
    "print(df_all_explanations.iloc[1]['explanation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
